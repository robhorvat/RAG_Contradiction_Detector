{
  "project": "RAG_Contradiction_Detector",
  "run_type": "eval_harness_v1",
  "generated_at_utc": "2026-02-17T19:47:10+00:00",
  "data": {
    "pairs_file": "data/scifact/processed/dev_pairs.jsonl",
    "scifact_claims_file": "data/scifact/raw/data/claims_dev.jsonl",
    "scifact_corpus_file": "data/scifact/raw/data/corpus.jsonl",
    "max_pairs": 0,
    "max_claims": 0,
    "seed": 42,
    "k_values": [
      1,
      3,
      5,
      10
    ]
  },
  "metrics": {
    "retrieval": {
      "n_queries": 188,
      "recall_at_k": {
        "1": 0.3989,
        "3": 0.5213,
        "5": 0.5479,
        "10": 0.6064
      },
      "mrr_at_k": {
        "1": 0.3989,
        "3": 0.4548,
        "5": 0.4604,
        "10": 0.4679
      },
      "status": "ok"
    },
    "verdict": {
      "n_pairs": 638,
      "labels": [
        "Contradictory",
        "Supporting",
        "Unrelated"
      ],
      "models": {
        "majority_unrelated": {
          "accuracy": 0.4702,
          "macro_f1": 0.2132,
          "per_label": {
            "Contradictory": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0,
              "support": 122
            },
            "Supporting": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0,
              "support": 216
            },
            "Unrelated": {
              "precision": 0.4702,
              "recall": 1.0,
              "f1": 0.6397,
              "support": 300
            }
          },
          "n_samples": 638,
          "contradiction_f1": 0.0
        },
        "heuristic": {
          "accuracy": 0.5784,
          "macro_f1": 0.4925,
          "per_label": {
            "Contradictory": {
              "precision": 0.3333,
              "recall": 0.1475,
              "f1": 0.2045,
              "support": 122
            },
            "Supporting": {
              "precision": 0.4905,
              "recall": 0.7176,
              "f1": 0.5827,
              "support": 216
            },
            "Unrelated": {
              "precision": 0.7313,
              "recall": 0.6533,
              "f1": 0.6901,
              "support": 300
            }
          },
          "n_samples": 638,
          "contradiction_f1": 0.2045
        },
        "torch_verifier": {
          "accuracy": 0.4843,
          "macro_f1": 0.2452,
          "per_label": {
            "Contradictory": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0,
              "support": 122
            },
            "Supporting": {
              "precision": 0.5556,
              "recall": 0.0463,
              "f1": 0.0855,
              "support": 216
            },
            "Unrelated": {
              "precision": 0.4823,
              "recall": 0.9967,
              "f1": 0.65,
              "support": 300
            }
          },
          "n_samples": 638,
          "contradiction_f1": 0.0
        }
      }
    }
  },
  "model_meta": {
    "majority_unrelated": {
      "status": "ok"
    },
    "heuristic": {
      "status": "ok"
    },
    "torch_verifier": {
      "status": "ok",
      "checkpoint_path": "artifacts/torch_verifier_quick/torch_nli_verifier.pt",
      "checkpoint_source": "registry-latest",
      "device": "cpu",
      "device_mode": "cpu-auto"
    }
  },
  "quality_gate": {
    "status": "fail",
    "candidate_model": "torch_verifier",
    "reason": "evaluated torch verifier against thresholds",
    "thresholds": {
      "macro_f1_min": 0.7,
      "delta_over_heuristic_min": 0.1
    },
    "checks": [
      {
        "name": "macro_f1_min",
        "actual": 0.2452,
        "threshold": 0.7,
        "passed": false
      },
      {
        "name": "delta_over_heuristic_min",
        "actual": -0.2473,
        "threshold": 0.1,
        "passed": false
      }
    ]
  }
}